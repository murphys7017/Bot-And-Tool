{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'data': [\n",
    "    {'nickname': 'å…³ä¼¯å…°', 'remark': 'å…³ä¼¯å…°', 'user_id': 474527445}, \n",
    "    {'nickname': 'ğŸğŸğŸ', 'remark': 'ğŸğŸğŸ', 'user_id': 577003680}, \n",
    "], 'message': '', 'retcode': 0, 'status': 'ok'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.ç”¨æˆ·æ¶ˆæ¯è‡³æ§åˆ¶å™¨DispatcherServlet\n",
    "2.DispatcherServletè¿›è¡ŒåŠ å·¥å¤„ç†åè°ƒç”¨å¤„ç†å™¨æ˜ å°„å™¨HandlerMappingã€‚\n",
    "3.HandlerMappingæ ¹æ®æ¶ˆæ¯æ‰¾åˆ°å…·ä½“çš„å¤„ç†å™¨ï¼Œç”Ÿæˆå“åº”æ¶ˆæ¯è¿”å›ç»™DispatcherServletã€‚\n",
    "4.DispatcherServletæ ¹æ®å¤„ç†å™¨Handlerè·å–å¤„ç†å™¨é€‚é…å™¨HandlerAdapteræ‰§è¡ŒHandlerAdapterå¤„ç†ä¸€ç³»åˆ—çš„æ“ä½œï¼Œå¦‚ï¼šå‚æ•°å°è£…ï¼Œæ•°æ®æ ¼å¼è½¬æ¢ï¼Œæ•°æ®éªŒè¯ç­‰æ“ä½œ\n",
    "5.æ‰§è¡Œå¤„ç†å™¨Handler(Controllerï¼Œä¹Ÿå«é¡µé¢æ§åˆ¶å™¨)ã€‚\n",
    "6.Handleræ‰§è¡Œå®Œæˆè¿”å›ModelAndView\n",
    "7.HandlerAdapterå°†Handleræ‰§è¡Œç»“æœModelAndViewè¿”å›åˆ°DispatcherServlet\n",
    "8.DispatcherServletå°†ModelAndViewä¼ ç»™ViewResloverè§†å›¾è§£æå™¨\n",
    "9.ViewResloverè§£æåè¿”å›å…·ä½“View\n",
    "10.DispatcherServletå¯¹Viewè¿›è¡Œæ¸²æŸ“è§†å›¾ï¼ˆå³å°†æ¨¡å‹æ•°æ®modelå¡«å……è‡³è§†å›¾ä¸­ï¼‰ã€‚\n",
    "11.DispatcherServletå“åº”ç”¨æˆ·ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'post_type': 'message', \n",
    "    'message_type': 'private', \n",
    "    'time': 1694183059, \n",
    "    'self_id': 2762018040, \n",
    "    'sub_type': 'friend', \n",
    "    'target_id': 2762018040, \n",
    "    'message': 'ä½ å¥½', \n",
    "    'raw_message': 'ä½ å¥½', \n",
    "    'font': 0, \n",
    "    'sender': {'age': 0, 'nickname': 'Aki-Polaris', 'sex': 'unknown', 'user_id': 815049548}, \n",
    "    'message_id': -2001115448, \n",
    "    'user_id': 815049548\n",
    "}\n",
    "message_info = {\n",
    "    'post_type': 'message', \n",
    "    'message_type': 'group', \n",
    "    'time': 1694395091, \n",
    "    'self_id': 2762018040, \n",
    "    'sub_type': 'normal', \n",
    "    'anonymous': None, \n",
    "    'message': '[CQ:image,file=138fd15bdabbcfb4c3daa148555fe447.image,subType=1,url=https://gchat.qpic.cn/gchatpic_new/1352402688/830954892-2582910460-138FD15BDABBCFB4C3DAA148555FE447/0?term=255&amp;is_origin=0]', \n",
    "    'message_seq': 81567, \n",
    "    'raw_message': '[CQ:image,file=138fd15bdabbcfb4c3daa148555fe447.image,subType=1,url=https://gchat.qpic.cn/gchatpic_new/1352402688/830954892-2582910460-138FD15BDABBCFB4C3DAA148555FE447/0?term=255&amp;is_origin=0]', \n",
    "    'font': 0, \n",
    "    'group_id': 830954892, \n",
    "    'sender': {'age': 0, 'area': '', 'card': '', 'level': '', 'nickname': 'ç¦»å²±å’Œè‘›é¥°åº”ä¸º', 'role': 'member', 'sex': 'unknown', 'title': '', 'user_id': 1352402688}, \n",
    "    'user_id': 1352402688, \n",
    "    'message_id': -185279243\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "ltp = LTP('LTP/small')\n",
    "message = 'æ˜å¤©å¤©æ€ä¹ˆæ ·å‘€'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.object_definition import Statement\n",
    "from service.MatchSys.utils import import_module\n",
    "\n",
    "\n",
    "class MessageAdapter(object):\n",
    "    \"\"\"\n",
    "    This is an abstract class that represents the interface\n",
    "    that all message adapters should implement.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        from service.MatchSys.object_definition import Statement\n",
    "        from ltp import LTP\n",
    "\n",
    "        # åˆå§‹åŒ–é¢„å¤„ç†ç¨‹åº\n",
    "        preprocessors = kwargs.get('preprocessors', ['jionlp.clean_text'])\n",
    "        self.preprocessors = []\n",
    "        for preprocessor in preprocessors:\n",
    "            self.preprocessors.append(import_module(preprocessor))\n",
    "        \n",
    "        model_path = kwargs.get('model_path', 'LTP/small')\n",
    "        self.ltp = LTP(model_path)\n",
    "    class AdapterMethodNotImplementedError(NotImplementedError):\n",
    "        \"\"\"\n",
    "        An exception to be raised when an adapter method has not been implemented.\n",
    "        Typically this indicates that the developer is expected to implement the\n",
    "        method in a subclass.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, message='This method must be overridden in a subclass method.'):\n",
    "            \"\"\"\n",
    "            Set the message for the exception.\n",
    "            \"\"\"\n",
    "            super().__init__(message)\n",
    "\n",
    "    def check(self, message):\n",
    "        if message is None or message == '':\n",
    "            raise self.ChatBotException(\n",
    "                'Either a statement object or a \"text\" keyword '\n",
    "                'argument is required. Neither was provided.'\n",
    "            )\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def process(self, message):\n",
    "        # read the message\n",
    "\n",
    "        # Get Text message\n",
    "        input_statement = self.text_process(text=message)\n",
    "\n",
    "        # Add Other Info\n",
    "\n",
    "        # è·å–Statement\n",
    "        raise self.AdapterMethodNotImplementedError()\n",
    "\n",
    "    def text_process(self, text,**kwargs):\n",
    "        \"\"\"Return Search Text\n",
    "\n",
    "        Args:\n",
    "            text (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: Statement\n",
    "        \"\"\"\n",
    "        # æ¸…ç†æ–‡æœ¬\n",
    "        for preprocessor in self.preprocessors:\n",
    "            text = preprocessor(text)\n",
    "        kwargs['text'] = text\n",
    "        # åˆ†è¯\n",
    "        result = self.ltp.pipeline('ä½ è§‰å¾—Aæ€ä¹ˆæ ·', tasks = [\"cws\",\"srl\"])\n",
    "        kwargs['search_text'] = result.cws\n",
    "        t = result.srl[0]\n",
    "        for item in result.srl:\n",
    "            if len(t['arguments']) > len(item['arguments']):\n",
    "                t = item\n",
    "        kwargs['intent'] = t\n",
    "\n",
    "        input_statement = Statement(**kwargs)\n",
    "        return input_statement\n",
    "\n",
    "    @property\n",
    "    def class_name(self):\n",
    "        \"\"\"\n",
    "        Return the name of the current logic adapter class.\n",
    "        This is typically used for logging and debugging.\n",
    "        \"\"\"\n",
    "        return str(self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMessageAdapter(MessageAdapter):\n",
    "    \"\"\"\n",
    "    This is an abstract class that represents the interface\n",
    "    that all message adapters should implement.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def process(self, message):\n",
    "        # read the message\n",
    "\n",
    "        # Get Text message\n",
    "        input_statement = self.text_process(text=message)\n",
    "\n",
    "        # Add Other Info\n",
    "\n",
    "        # è·å–Statement\n",
    "        return input_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_adapter = TextMessageAdapter(model_path=r'D:\\Code\\MyLongTimeProject\\A\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.trainers import Trainer\n",
    "from service.MatchSys.utils import print_progress_bar\n",
    "\n",
    "class QATrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows a chat bot to be trained using a list of strings\n",
    "    where the list represents a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, conversation, **kwargs):\n",
    "        \"\"\"\n",
    "        {Q:[A1,A2...]}\n",
    "        Train the chat bot based on the provided list of\n",
    "        statements that represents a single conversation.\n",
    "        \"\"\"\n",
    "        source = kwargs.get('source', 'TRAIN_DATA')\n",
    "        conversation_text = kwargs.get('conversation', 'TRAIN_DATA')\n",
    "        statements_to_create = []\n",
    "        for index,Q in enumerate(conversation):\n",
    "            if self.show_training_progress:\n",
    "                print_progress_bar(\n",
    "                    'QA Trainer',\n",
    "                    index + 1, len(conversation)\n",
    "                )\n",
    "    \n",
    "            statement = self.chatbot.message_adapter.process(Q)\n",
    "            statement.next_id=-statement.id\n",
    "            statement.conversation=conversation_text\n",
    "            statement.type_of='Q'\n",
    "            statement.source=source\n",
    "            statement.persona='user'\n",
    "            statements_to_create.append(statement)\n",
    "            for A in conversation[Q]:\n",
    "                statement = self.chatbot.message_adapter.process(A)\n",
    "                statement.previous_id=-statement.id\n",
    "                statement.conversation=conversation_text\n",
    "                statement.type_of='A'\n",
    "                statement.source=source\n",
    "                statement.persona='bot:'+self.chatbot.name\n",
    "                statements_to_create.append(statement)\n",
    "        self.chatbot.storage.create_many(statements_to_create)\n",
    "        self.chatbot.docvector_tool.train(statements_to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.match_sys import MatchSys\n",
    "\n",
    "\n",
    "matchsys = MatchSys(name='Alice',model_path=r'D:\\Code\\MyLongTimeProject\\A\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qatrain = QATrainer(matchsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv(r'data1.txt',\n",
    "                sep=\"$\",\n",
    "                engine='python'\n",
    "                )\n",
    "a.head(5)\n",
    "a.to_csv(r'data1.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':[1,2,3], 'b':[4,5,6], 'c':[7,8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('a', [1, 2, 3])\n",
      "1\n",
      "('b', [4, 5, 6])\n",
      "2\n",
      "('c', [7, 8])\n"
     ]
    }
   ],
   "source": [
    "for index,k in enumerate(a.items()):\n",
    "    print(index)\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "map = {}\n",
    "# data = pd.read_excel(r'C:\\Users\\Administrator\\Documents\\GitHub\\QQ-Bot-And-Tool\\data\\FixedReply\\å‚²å¨‡ç³»äºŒæ¬¡å…ƒbotè¯åº“5åƒè¯V1.2.xlsx',header=None, sheet_name=0)\n",
    "data = pd.read_excel(r\"data\\FixedReply\\Test-1K.xlsx\",header=None, sheet_name=0)\n",
    "for index,row in data.iterrows():\n",
    "    row[0] = str(row[0]).strip()\n",
    "    row[1] = str(row[1]).strip()\n",
    "    if row[0] in map:\n",
    "        map[row[0]].append(row[1])\n",
    "    else:\n",
    "        map[row[0]] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissSearch(object):\n",
    "    def __init__(self,**kwargs):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.vector_model = SentenceTransformer('data/text2vec-base-chinese')\n",
    "        self.nlist = 50\n",
    "        self.index_search = None\n",
    "        self.signal_file_size = 10000\n",
    "    def data_process_2_vector(self,**kwargs):\n",
    "        data_list = kwargs.get('data_list',None)\n",
    "        file_list = kwargs.get('file_list',None)\n",
    "        # åŠ è½½æ¨¡å‹ï¼Œå°†æ•°æ®è¿›è¡Œå‘é‡åŒ–å¤„ç†\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer('data/text2vec-base-chinese')\n",
    "        sentences = data_list\n",
    "        sentence_embeddings = self.vector_model.encode(data_list)\n",
    "        # å°†å‘é‡å¤„ç†ç»“æœå­˜å‚¨\n",
    "        import numpy as np\n",
    "        import os\n",
    "        save_file = \"data/vec_data.npy\"\n",
    "        np.save(save_file, sentence_embeddings)\n",
    "\n",
    "        file_size = os.path.getsize(save_file)\n",
    "        print(\"%7.3f MB\" % (file_size/1024/1024))\n",
    "    def build_faiss_index(self):\n",
    "        # åŠ è½½é¢„å¤„ç†æ•°æ®\n",
    "        import numpy as np\n",
    "        sentences = list(map.keys())\n",
    "        sentence_embeddings = np.load(\"data/vec_data.npy\")\n",
    "        print(\"è½½å…¥å‘é‡æ•°æ®å®Œæ¯•ï¼Œæ•°æ®é‡\", len(sentence_embeddings))\n",
    "\n",
    "        # ä½¿ç”¨æ„å»ºå‘é‡æ—¶çš„æ¨¡å‹æ¥æ„å»ºå‘é‡ç´¢å¼•\n",
    "        import faiss\n",
    "        dimension = sentence_embeddings.shape[1]\n",
    "        quantizer = faiss.IndexFlatL2(dimension)\n",
    "        self.index_search = faiss.IndexIVFFlat(quantizer, dimension, self.nlist)\n",
    "        index.train(sentence_embeddings)\n",
    "        index.add(sentence_embeddings)\n",
    "        print(\"å»ºç«‹å‘é‡ç´¢å¼•å®Œæ¯•ï¼Œæ•°æ®é‡\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è½½å…¥å‘é‡æ•°æ®å®Œæ¯•ï¼Œæ•°æ®é‡ 976\n",
      "å»ºç«‹å‘é‡ç´¢å¼•å®Œæ¯•ï¼Œæ•°æ®é‡ 976\n",
      "è½½å…¥æ¨¡å‹å®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½é¢„å¤„ç†æ•°æ®\n",
    "import numpy as np\n",
    "sentences = list(map.keys())\n",
    "sentence_embeddings = np.load(\"data/vec_data.npy\")\n",
    "print(\"è½½å…¥å‘é‡æ•°æ®å®Œæ¯•ï¼Œæ•°æ®é‡\", len(sentence_embeddings))\n",
    "\n",
    "# ä½¿ç”¨æ„å»ºå‘é‡æ—¶çš„æ¨¡å‹æ¥æ„å»ºå‘é‡ç´¢å¼•\n",
    "import faiss\n",
    "dimension = sentence_embeddings.shape[1]\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "nlist = 50\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "index.train(sentence_embeddings)\n",
    "index.add(sentence_embeddings)\n",
    "print(\"å»ºç«‹å‘é‡ç´¢å¼•å®Œæ¯•ï¼Œæ•°æ®é‡\", index.ntotal)\n",
    "\n",
    "# å°è¯•è¿›è¡ŒæŸ¥è¯¢\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('data/text2vec-base-chinese')\n",
    "print(\"è½½å…¥æ¨¡å‹å®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = 10\n",
    "search = model.encode([\"å¥½ç´¯å•Š\"])\n",
    "D, I = index.search(search, topK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9502930e-10 4.5865227e+01 8.8828560e+01 1.0693193e+02 1.2101427e+02\n",
      "  1.5374112e+02 3.4294498e+02 3.7847754e+02 3.4028235e+38 3.4028235e+38]]\n",
      "[[ 33 632 589 605 531 330  76 825  -1  -1]]\n"
     ]
    }
   ],
   "source": [
    "print(D)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½ç´¯å•Š\n",
      "å¥½ç´¯å“¦\n",
      "å¥½ç´¯\n",
      "æˆ‘å¥½ç´¯\n",
      "æˆ‘ç´¯äº†\n",
      "ç´¯å—\n",
      "æˆ‘èƒŒä½ \n",
      "æˆ‘è¦è†æ•\n"
     ]
    }
   ],
   "source": [
    "for i in I[0]:\n",
    "    if i != -1:\n",
    "        ret = list(map.keys())[i]\n",
    "        print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®ç±»å‹ <class 'numpy.ndarray'>\n",
      "æ•°ç»„å…ƒç´ æ•°æ®ç±»å‹ï¼š float32\n",
      "æ•°ç»„å…ƒç´ æ€»æ•°ï¼š 749568\n",
      "æ•°ç»„å½¢çŠ¶ï¼š (976, 768)\n",
      "æ•°ç»„çš„ç»´åº¦æ•°ç›® 2\n"
     ]
    }
   ],
   "source": [
    "print(\"æ•°æ®ç±»å‹\",type(sentence_embeddings))           #æ‰“å°æ•°ç»„æ•°æ®ç±»å‹  \n",
    "print(\"æ•°ç»„å…ƒç´ æ•°æ®ç±»å‹ï¼š\",sentence_embeddings.dtype) #æ‰“å°æ•°ç»„å…ƒç´ æ•°æ®ç±»å‹  \n",
    "print(\"æ•°ç»„å…ƒç´ æ€»æ•°ï¼š\",sentence_embeddings.size)      #æ‰“å°æ•°ç»„å°ºå¯¸ï¼Œå³æ•°ç»„å…ƒç´ æ€»æ•°  \n",
    "print(\"æ•°ç»„å½¢çŠ¶ï¼š\",sentence_embeddings.shape)         #æ‰“å°æ•°ç»„å½¢çŠ¶  \n",
    "print(\"æ•°ç»„çš„ç»´åº¦æ•°ç›®\",sentence_embeddings.ndim)      #æ‰“å°æ•°ç»„çš„ç»´åº¦æ•°ç›®  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2970518 , -0.25346562, -0.23536594, ..., -0.6353451 ,\n",
       "         0.36731493,  0.02351565],\n",
       "       [-0.54003495, -0.79583836,  0.38487688, ..., -0.4618318 ,\n",
       "        -1.2059765 , -0.12932391],\n",
       "       [ 0.32315907,  0.10518757, -0.52833754, ...,  0.43228775,\n",
       "        -0.9100478 , -0.8505187 ],\n",
       "       ...,\n",
       "       [-0.2751668 ,  0.65258443,  0.08361594, ...,  0.22980374,\n",
       "        -1.2248287 , -0.50112844],\n",
       "       [ 0.7900612 , -0.63520163,  0.25002787, ..., -0.710253  ,\n",
       "        -0.94551253, -0.39789113],\n",
       "       [ 0.5351167 ,  0.34678018,  1.0117722 , ..., -0.03430998,\n",
       "         0.1018568 ,  0.01111399]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
