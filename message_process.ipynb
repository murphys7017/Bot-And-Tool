{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'data': [\n",
    "    {'nickname': 'ÂÖ≥‰ºØÂÖ∞', 'remark': 'ÂÖ≥‰ºØÂÖ∞', 'user_id': 474527445}, \n",
    "    {'nickname': 'üçÅüçÅüçÅ', 'remark': 'üçÅüçÅüçÅ', 'user_id': 577003680}, \n",
    "], 'message': '', 'retcode': 0, 'status': 'ok'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Áî®Êà∑Ê∂àÊÅØËá≥ÊéßÂà∂Âô®DispatcherServlet\n",
    "2.DispatcherServletËøõË°åÂä†Â∑•Â§ÑÁêÜÂêéË∞ÉÁî®Â§ÑÁêÜÂô®Êò†Â∞ÑÂô®HandlerMapping„ÄÇ\n",
    "3.HandlerMappingÊ†πÊçÆÊ∂àÊÅØÊâæÂà∞ÂÖ∑‰ΩìÁöÑÂ§ÑÁêÜÂô®ÔºåÁîüÊàêÂìçÂ∫îÊ∂àÊÅØËøîÂõûÁªôDispatcherServlet„ÄÇ\n",
    "4.DispatcherServletÊ†πÊçÆÂ§ÑÁêÜÂô®HandlerËé∑ÂèñÂ§ÑÁêÜÂô®ÈÄÇÈÖçÂô®HandlerAdapterÊâßË°åHandlerAdapterÂ§ÑÁêÜ‰∏ÄÁ≥ªÂàóÁöÑÊìç‰ΩúÔºåÂ¶ÇÔºöÂèÇÊï∞Â∞ÅË£ÖÔºåÊï∞ÊçÆÊ†ºÂºèËΩ¨Êç¢ÔºåÊï∞ÊçÆÈ™åËØÅÁ≠âÊìç‰Ωú\n",
    "5.ÊâßË°åÂ§ÑÁêÜÂô®Handler(ControllerÔºå‰πüÂè´È°µÈù¢ÊéßÂà∂Âô®)„ÄÇ\n",
    "6.HandlerÊâßË°åÂÆåÊàêËøîÂõûModelAndView\n",
    "7.HandlerAdapterÂ∞ÜHandlerÊâßË°åÁªìÊûúModelAndViewËøîÂõûÂà∞DispatcherServlet\n",
    "8.DispatcherServletÂ∞ÜModelAndView‰º†ÁªôViewResloverËßÜÂõæËß£ÊûêÂô®\n",
    "9.ViewResloverËß£ÊûêÂêéËøîÂõûÂÖ∑‰ΩìView\n",
    "10.DispatcherServletÂØπViewËøõË°åÊ∏≤ÊüìËßÜÂõæÔºàÂç≥Â∞ÜÊ®°ÂûãÊï∞ÊçÆmodelÂ°´ÂÖÖËá≥ËßÜÂõæ‰∏≠Ôºâ„ÄÇ\n",
    "11.DispatcherServletÂìçÂ∫îÁî®Êà∑„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'post_type': 'message', \n",
    "    'message_type': 'private', \n",
    "    'time': 1694183059, \n",
    "    'self_id': 2762018040, \n",
    "    'sub_type': 'friend', \n",
    "    'target_id': 2762018040, \n",
    "    'message': '‰Ω†Â•Ω', \n",
    "    'raw_message': '‰Ω†Â•Ω', \n",
    "    'font': 0, \n",
    "    'sender': {'age': 0, 'nickname': 'Aki-Polaris', 'sex': 'unknown', 'user_id': 815049548}, \n",
    "    'message_id': -2001115448, \n",
    "    'user_id': 815049548\n",
    "}\n",
    "message_info = {\n",
    "    'post_type': 'message', \n",
    "    'message_type': 'group', \n",
    "    'time': 1694395091, \n",
    "    'self_id': 2762018040, \n",
    "    'sub_type': 'normal', \n",
    "    'anonymous': None, \n",
    "    'message': '[CQ:image,file=138fd15bdabbcfb4c3daa148555fe447.image,subType=1,url=https://gchat.qpic.cn/gchatpic_new/1352402688/830954892-2582910460-138FD15BDABBCFB4C3DAA148555FE447/0?term=255&amp;is_origin=0]', \n",
    "    'message_seq': 81567, \n",
    "    'raw_message': '[CQ:image,file=138fd15bdabbcfb4c3daa148555fe447.image,subType=1,url=https://gchat.qpic.cn/gchatpic_new/1352402688/830954892-2582910460-138FD15BDABBCFB4C3DAA148555FE447/0?term=255&amp;is_origin=0]', \n",
    "    'font': 0, \n",
    "    'group_id': 830954892, \n",
    "    'sender': {'age': 0, 'area': '', 'card': '', 'level': '', 'nickname': 'Á¶ªÂ≤±ÂíåËëõÈ•∞Â∫î‰∏∫', 'role': 'member', 'sex': 'unknown', 'title': '', 'user_id': 1352402688}, \n",
    "    'user_id': 1352402688, \n",
    "    'message_id': -185279243\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "ltp = LTP('LTP/small')\n",
    "message = 'ÊòéÂ§©Â§©ÊÄé‰πàÊ†∑ÂëÄ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.object_definition import Statement\n",
    "from service.MatchSys.utils import import_module\n",
    "\n",
    "\n",
    "class MessageAdapter(object):\n",
    "    \"\"\"\n",
    "    This is an abstract class that represents the interface\n",
    "    that all message adapters should implement.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        from service.MatchSys.object_definition import Statement\n",
    "        from ltp import LTP\n",
    "\n",
    "        # ÂàùÂßãÂåñÈ¢ÑÂ§ÑÁêÜÁ®ãÂ∫è\n",
    "        preprocessors = kwargs.get('preprocessors', ['jionlp.clean_text'])\n",
    "        self.preprocessors = []\n",
    "        for preprocessor in preprocessors:\n",
    "            self.preprocessors.append(import_module(preprocessor))\n",
    "        \n",
    "        model_path = kwargs.get('model_path', 'LTP/small')\n",
    "        self.ltp = LTP(model_path)\n",
    "    class AdapterMethodNotImplementedError(NotImplementedError):\n",
    "        \"\"\"\n",
    "        An exception to be raised when an adapter method has not been implemented.\n",
    "        Typically this indicates that the developer is expected to implement the\n",
    "        method in a subclass.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, message='This method must be overridden in a subclass method.'):\n",
    "            \"\"\"\n",
    "            Set the message for the exception.\n",
    "            \"\"\"\n",
    "            super().__init__(message)\n",
    "\n",
    "    def check(self, message):\n",
    "        if message is None or message == '':\n",
    "            raise self.ChatBotException(\n",
    "                'Either a statement object or a \"text\" keyword '\n",
    "                'argument is required. Neither was provided.'\n",
    "            )\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def process(self, message):\n",
    "        # read the message\n",
    "\n",
    "        # Get Text message\n",
    "        input_statement = self.text_process(text=message)\n",
    "\n",
    "        # Add Other Info\n",
    "\n",
    "        # Ëé∑ÂèñStatement\n",
    "        raise self.AdapterMethodNotImplementedError()\n",
    "\n",
    "    def text_process(self, text,**kwargs):\n",
    "        \"\"\"Return Search Text\n",
    "\n",
    "        Args:\n",
    "            text (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: Statement\n",
    "        \"\"\"\n",
    "        # Ê∏ÖÁêÜÊñáÊú¨\n",
    "        for preprocessor in self.preprocessors:\n",
    "            text = preprocessor(text)\n",
    "        kwargs['text'] = text\n",
    "        # ÂàÜËØç\n",
    "        result = self.ltp.pipeline('‰Ω†ËßâÂæóAÊÄé‰πàÊ†∑', tasks = [\"cws\",\"srl\"])\n",
    "        kwargs['search_text'] = result.cws\n",
    "        t = result.srl[0]\n",
    "        for item in result.srl:\n",
    "            if len(t['arguments']) > len(item['arguments']):\n",
    "                t = item\n",
    "        kwargs['intent'] = t\n",
    "\n",
    "        input_statement = Statement(**kwargs)\n",
    "        return input_statement\n",
    "\n",
    "    @property\n",
    "    def class_name(self):\n",
    "        \"\"\"\n",
    "        Return the name of the current logic adapter class.\n",
    "        This is typically used for logging and debugging.\n",
    "        \"\"\"\n",
    "        return str(self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMessageAdapter(MessageAdapter):\n",
    "    \"\"\"\n",
    "    This is an abstract class that represents the interface\n",
    "    that all message adapters should implement.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def process(self, message):\n",
    "        # read the message\n",
    "\n",
    "        # Get Text message\n",
    "        input_statement = self.text_process(text=message)\n",
    "\n",
    "        # Add Other Info\n",
    "\n",
    "        # Ëé∑ÂèñStatement\n",
    "        return input_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_adapter = TextMessageAdapter(model_path=r'D:\\Code\\MyLongTimeProject\\A\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.trainers import Trainer\n",
    "from service.MatchSys.utils import print_progress_bar\n",
    "\n",
    "class QATrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows a chat bot to be trained using a list of strings\n",
    "    where the list represents a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, conversation, **kwargs):\n",
    "        \"\"\"\n",
    "        {Q:[A1,A2...]}\n",
    "        Train the chat bot based on the provided list of\n",
    "        statements that represents a single conversation.\n",
    "        \"\"\"\n",
    "        source = kwargs.get('source', 'TRAIN_DATA')\n",
    "        conversation_text = kwargs.get('conversation', 'TRAIN_DATA')\n",
    "        statements_to_create = []\n",
    "        for index,Q in enumerate(conversation):\n",
    "            if self.show_training_progress:\n",
    "                print_progress_bar(\n",
    "                    'QA Trainer',\n",
    "                    index + 1, len(conversation)\n",
    "                )\n",
    "    \n",
    "            statement = self.chatbot.message_adapter.process(Q)\n",
    "            statement.next_id=-statement.id\n",
    "            statement.conversation=conversation_text\n",
    "            statement.type_of='Q'\n",
    "            statement.source=source\n",
    "            statement.persona='user'\n",
    "            statements_to_create.append(statement)\n",
    "            for A in conversation[Q]:\n",
    "                statement = self.chatbot.message_adapter.process(A)\n",
    "                statement.previous_id=-statement.id\n",
    "                statement.conversation=conversation_text\n",
    "                statement.type_of='A'\n",
    "                statement.source=source\n",
    "                statement.persona='bot:'+self.chatbot.name\n",
    "                statements_to_create.append(statement)\n",
    "        self.chatbot.storage.create_many(statements_to_create)\n",
    "        self.chatbot.docvector_tool.train(statements_to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.match_sys import MatchSys\n",
    "\n",
    "\n",
    "matchsys = MatchSys(name='Alice',model_path=r'D:\\Code\\MyLongTimeProject\\A\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qatrain = QATrainer(matchsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv(r'data1.txt',\n",
    "                sep=\"$\",\n",
    "                engine='python'\n",
    "                )\n",
    "a.head(5)\n",
    "a.to_csv(r'data1.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':[1,2,3], 'b':[4,5,6], 'c':[7,8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,k in enumerate(a.items()):\n",
    "    print(index)\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "map = {}\n",
    "# data = pd.read_excel(r'C:\\Users\\Administrator\\Documents\\GitHub\\QQ-Bot-And-Tool\\data\\FixedReply\\ÂÇ≤Â®áÁ≥ª‰∫åÊ¨°ÂÖÉbotËØçÂ∫ì5ÂçÉËØçV1.2.xlsx',header=None, sheet_name=0)\n",
    "data = pd.read_excel(r\"data\\FixedReply\\Test-1K.xlsx\",header=None, sheet_name=0)\n",
    "for index,row in data.iterrows():\n",
    "    row[0] = str(row[0]).strip()\n",
    "    row[1] = str(row[1]).strip()\n",
    "    if row[0] in map:\n",
    "        map[row[0]].append(row[1])\n",
    "    else:\n",
    "        map[row[0]] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel(r\"data\\FixedReply\\Test-1K.xlsx\",header=None, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "# set HNSW index parameters\n",
    "M = 64  # number of connections each vertex will have\n",
    "ef_search = 32  # depth of layers explored during search\n",
    "ef_construction = 64  # depth of layers explored during index construction\n",
    "\n",
    "# initialize index (d == 128)\n",
    "index = faiss.IndexHNSWFlat(768, M)\n",
    "# set efConstruction and efSearch parameters\n",
    "index.hnsw.efConstruction = ef_construction\n",
    "index.hnsw.efSearch = ef_search\n",
    "# add data to index\n",
    "sentence_embeddings = np.load(\"data/vec_data.npy\")\n",
    "sentence_embeddings = sentence_embeddings.astype(np.float32)\n",
    "index.add(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import time\n",
    "\n",
    "class FaissSearch(object):\n",
    "    def __init__(self,**kwargs):\n",
    "        \n",
    "        self.vector_model = SentenceTransformer('data/text2vec-base-chinese')\n",
    "        self.nlist = 50\n",
    "        self.hnsw_index = None\n",
    "        self.signal_file_size = 10000\n",
    "        self.data_d = 128\n",
    "    def get_ids(self,data_list):\n",
    "        ids = []\n",
    "        for index,value in enumerate(data_list):\n",
    "            ids.append(index)\n",
    "        # Â∞ÜÂêëÈáèÂ§ÑÁêÜÁªìÊûúÂ≠òÂÇ®\n",
    "        ids_file = \"data/Faiss/ids_data.npy\"\n",
    "        np.save(ids_file, np.array(ids))\n",
    "        file_size = os.path.getsize(ids_file)\n",
    "        print(\"%7.3f MB\" % (file_size/1024/1024))\n",
    "    def data_process_10w(self,**kwargs):\n",
    "        data_save_path = kwargs.get('data_save_path',\"data/Faiss/vector_data\") \n",
    "        sentences = kwargs.get('data_list',None)\n",
    "        # Âä†ËΩΩÊ®°ÂûãÔºåÂ∞ÜÊï∞ÊçÆËøõË°åÂêëÈáèÂåñÂ§ÑÁêÜ\n",
    "        sentence_embeddings = self.vector_model.encode(sentences)\n",
    "        # Â∞ÜÂêëÈáèÂ§ÑÁêÜÁªìÊûúÂ≠òÂÇ®\n",
    "        n, d = sentence_embeddings.shape\n",
    "        sentence_embeddings_file_name = n+\"-\"+d+\"-\"+time.time()+\"-vec_data.npy\"\n",
    "        np.save(os.path.join(data_save_path,sentence_embeddings_file_name), sentence_embeddings)\n",
    "        file_size = os.path.getsize(os.path.join(data_save_path,sentence_embeddings_file_name))\n",
    "        print(sentence_embeddings_file_name + \" %7.3f MB\" % (file_size/1024/1024))\n",
    "    def data_process_2_vector(self,**kwargs):\n",
    "        data_save_path = kwargs.get('data_save_path',\"data/Faiss/vector_data\") \n",
    "        data_list = kwargs.get('data_list',None)\n",
    "        file_list = kwargs.get('file_list',None)\n",
    "        if data_list:\n",
    "            # Âä†ËΩΩÊ®°ÂûãÔºåÂ∞ÜÊï∞ÊçÆËøõË°åÂêëÈáèÂåñÂ§ÑÁêÜ\n",
    "            sentences = data_list\n",
    "            if len(sentences) <= 100000:\n",
    "                self.data_process_10w(**kwargs)\n",
    "            else:\n",
    "                for i in range(0, len(sentences), 100000):\n",
    "                    kwargs['data_list'] = sentences[i: i + 100000]\n",
    "                    self.data_process_10w(**kwargs)\n",
    "\n",
    "        elif file_list:\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\"\n",
    "    def build_hnsw(self,vec_path='data/Faiss/vec_data'):\n",
    "        sentence_embeddings_list = []\n",
    "        files = []\n",
    "        for file in os.listdir(vec_path):\n",
    "            if file.endswith('.npy'):\n",
    "                files.append(os.path.join(vec_path,file))\n",
    "                sentence_embeddings = np.load(os.path.join(vec_path,file))\n",
    "                sentence_embeddings = sentence_embeddings.astype(np.float32)\n",
    "                sentence_embeddings_list.append(sentence_embeddings)\n",
    "        \n",
    "        n,d = sentence_embeddings_list[0].shape\n",
    "        # set HNSW index parameters\n",
    "        M = 64  # number of connections each vertex will have Ë∂äÂ§ßÊêúÁ¥¢Êó∂Èó¥Ë∂äÈïø\n",
    "        ef_search = 32  # depth of layers explored during search Ë∂äÂ§ßÊêúÁ¥¢Êó∂Èó¥Ë∂äÈïø\n",
    "        ef_construction = 64  # depth of layers explored during index construction Ë∂äÂ§ßÊûÑÂª∫Êó∂Èó¥Ë∂äÈïø\n",
    "\n",
    "        # initialize index (d == 128)\n",
    "        self.hnsw_index = faiss.IndexHNSWFlat(d, M)\n",
    "        # set efConstruction and efSearch parameters\n",
    "        self.hnsw_index.hnsw.efConstruction = ef_construction\n",
    "        self.hnsw_index.hnsw.efSearch = ef_search\n",
    "        # self.hnsw_index = faiss.IndexIDMap2(self.hnsw_index)\n",
    "        \n",
    "        self.add_data(vec_path)\n",
    "    def add_data(self,vec_path='data/Faiss/vec_data.npy'):\n",
    "        sentence_embeddings = np.load(vec_path)\n",
    "        sentence_embeddings = sentence_embeddings.astype(np.float32)\n",
    "        print(\"ËΩΩÂÖ•ÂêëÈáèÊï∞ÊçÆÂÆåÊØïÔºåÊï∞ÊçÆÈáè\", len(sentence_embeddings))\n",
    "        self.hnsw_index.add(sentence_embeddings)\n",
    "    def search(self,sentences,topk=1):\n",
    "        if isinstance(sentences,list):\n",
    "            pass\n",
    "        else:\n",
    "            sentences = [sentences]\n",
    "        sentence_embeddings = self.vector_model.encode(sentences)\n",
    "        return self.hnsw_index.search(sentence_embeddings,topk)\n",
    "faissSearch = FaissSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17.156 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel(r\"data\\FixedReply\\ÂÇ≤Â®áÁ≥ª‰∫åÊ¨°ÂÖÉbotËØçÂ∫ì5ÂçÉËØçV1.2.xlsx\",header=None, sheet_name=0)\n",
    "faissSearch.data_process_2_vector(data_list=data[0].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ËΩΩÂÖ•ÂêëÈáèÊï∞ÊçÆÂÆåÊØïÔºåÊï∞ÊçÆÈáè 5856\n"
     ]
    }
   ],
   "source": [
    "faissSearch.add_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14981, 768)\n",
      "ËΩΩÂÖ•ÂêëÈáèÊï∞ÊçÆÂÆåÊØïÔºåÊï∞ÊçÆÈáè 14981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "faissSearch.build_hnsw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(faissSearch.hnsw_index,'data/Faiss/hnws.indx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.5935717e-10]], dtype=float32), array([[15024]], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faissSearch.search(['ÊàëÂñúÊ¨¢‰Ω†'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âä†ËΩΩÈ¢ÑÂ§ÑÁêÜÊï∞ÊçÆ\n",
    "import numpy as np\n",
    "sentences = list(map.keys())\n",
    "sentence_embeddings = np.load(\"data/vec_data.npy\")\n",
    "print(\"ËΩΩÂÖ•ÂêëÈáèÊï∞ÊçÆÂÆåÊØïÔºåÊï∞ÊçÆÈáè\", len(sentence_embeddings))\n",
    "\n",
    "# ‰ΩøÁî®ÊûÑÂª∫ÂêëÈáèÊó∂ÁöÑÊ®°ÂûãÊù•ÊûÑÂª∫ÂêëÈáèÁ¥¢Âºï\n",
    "import faiss\n",
    "dimension = sentence_embeddings.shape[1]\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "nlist = 50\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "index.train(sentence_embeddings)\n",
    "index.add(sentence_embeddings)\n",
    "print(\"Âª∫Á´ãÂêëÈáèÁ¥¢ÂºïÂÆåÊØïÔºåÊï∞ÊçÆÈáè\", index.ntotal)\n",
    "\n",
    "# Â∞ùËØïËøõË°åÊü•ËØ¢\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('data/text2vec-base-chinese')\n",
    "print(\"ËΩΩÂÖ•Ê®°ÂûãÂÆåÊØï\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = 10\n",
    "search = model.encode([\"Â•ΩÁ¥ØÂïä\"])\n",
    "D, I = index.search(search, topK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in I[0]:\n",
    "    if i != -1:\n",
    "        ret = list(map.keys())[i]\n",
    "        print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Êï∞ÊçÆÁ±ªÂûã\",type(sentence_embeddings))           #ÊâìÂç∞Êï∞ÁªÑÊï∞ÊçÆÁ±ªÂûã  \n",
    "print(\"Êï∞ÁªÑÂÖÉÁ¥†Êï∞ÊçÆÁ±ªÂûãÔºö\",sentence_embeddings.dtype) #ÊâìÂç∞Êï∞ÁªÑÂÖÉÁ¥†Êï∞ÊçÆÁ±ªÂûã  \n",
    "print(\"Êï∞ÁªÑÂÖÉÁ¥†ÊÄªÊï∞Ôºö\",sentence_embeddings.size)      #ÊâìÂç∞Êï∞ÁªÑÂ∞∫ÂØ∏ÔºåÂç≥Êï∞ÁªÑÂÖÉÁ¥†ÊÄªÊï∞  \n",
    "print(\"Êï∞ÁªÑÂΩ¢Áä∂Ôºö\",sentence_embeddings.shape)         #ÊâìÂç∞Êï∞ÁªÑÂΩ¢Áä∂  \n",
    "print(\"Êï∞ÁªÑÁöÑÁª¥Â∫¶Êï∞ÁõÆ\",sentence_embeddings.ndim)      #ÊâìÂç∞Êï∞ÁªÑÁöÑÁª¥Â∫¶Êï∞ÁõÆ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hnws.indx\n",
      "vec_data.npy\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('data/Faiss/'):\n",
    "    print(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
