{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'data': [\n",
    "    {'nickname': 'å…³ä¼¯å…°', 'remark': 'å…³ä¼¯å…°', 'user_id': 474527445}, \n",
    "    {'nickname': 'ğŸğŸğŸ', 'remark': 'ğŸğŸğŸ', 'user_id': 577003680}, \n",
    "], 'message': '', 'retcode': 0, 'status': 'ok'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.ç”¨æˆ·æ¶ˆæ¯è‡³æ§åˆ¶å™¨DispatcherServlet\n",
    "2.DispatcherServletè¿›è¡ŒåŠ å·¥å¤„ç†åè°ƒç”¨å¤„ç†å™¨æ˜ å°„å™¨HandlerMappingã€‚\n",
    "3.HandlerMappingæ ¹æ®æ¶ˆæ¯æ‰¾åˆ°å…·ä½“çš„å¤„ç†å™¨ï¼Œç”Ÿæˆå“åº”æ¶ˆæ¯è¿”å›ç»™DispatcherServletã€‚\n",
    "4.DispatcherServletæ ¹æ®å¤„ç†å™¨Handlerè·å–å¤„ç†å™¨é€‚é…å™¨HandlerAdapteræ‰§è¡ŒHandlerAdapterå¤„ç†ä¸€ç³»åˆ—çš„æ“ä½œï¼Œå¦‚ï¼šå‚æ•°å°è£…ï¼Œæ•°æ®æ ¼å¼è½¬æ¢ï¼Œæ•°æ®éªŒè¯ç­‰æ“ä½œ\n",
    "5.æ‰§è¡Œå¤„ç†å™¨Handler(Controllerï¼Œä¹Ÿå«é¡µé¢æ§åˆ¶å™¨)ã€‚\n",
    "6.Handleræ‰§è¡Œå®Œæˆè¿”å›ModelAndView\n",
    "7.HandlerAdapterå°†Handleræ‰§è¡Œç»“æœModelAndViewè¿”å›åˆ°DispatcherServlet\n",
    "8.DispatcherServletå°†ModelAndViewä¼ ç»™ViewResloverè§†å›¾è§£æå™¨\n",
    "9.ViewResloverè§£æåè¿”å›å…·ä½“View\n",
    "10.DispatcherServletå¯¹Viewè¿›è¡Œæ¸²æŸ“è§†å›¾ï¼ˆå³å°†æ¨¡å‹æ•°æ®modelå¡«å……è‡³è§†å›¾ä¸­ï¼‰ã€‚\n",
    "11.DispatcherServletå“åº”ç”¨æˆ·ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'post_type': 'message', \n",
    "    'message_type': 'private', \n",
    "    'time': 1694183059, \n",
    "    'self_id': 2762018040, \n",
    "    'sub_type': 'friend', \n",
    "    'target_id': 2762018040, \n",
    "    'message': 'ä½ å¥½', \n",
    "    'raw_message': 'ä½ å¥½', \n",
    "    'font': 0, \n",
    "    'sender': {'age': 0, 'nickname': 'Aki-Polaris', 'sex': 'unknown', 'user_id': 815049548}, \n",
    "    'message_id': -2001115448, \n",
    "    'user_id': 815049548\n",
    "}\n",
    "message_info = {\n",
    "    'post_type': 'message', \n",
    "    'message_type': 'group', \n",
    "    'time': 1694395091, \n",
    "    'self_id': 2762018040, \n",
    "    'sub_type': 'normal', \n",
    "    'anonymous': None, \n",
    "    'message': '[CQ:image,file=138fd15bdabbcfb4c3daa148555fe447.image,subType=1,url=https://gchat.qpic.cn/gchatpic_new/1352402688/830954892-2582910460-138FD15BDABBCFB4C3DAA148555FE447/0?term=255&amp;is_origin=0]', \n",
    "    'message_seq': 81567, \n",
    "    'raw_message': '[CQ:image,file=138fd15bdabbcfb4c3daa148555fe447.image,subType=1,url=https://gchat.qpic.cn/gchatpic_new/1352402688/830954892-2582910460-138FD15BDABBCFB4C3DAA148555FE447/0?term=255&amp;is_origin=0]', \n",
    "    'font': 0, \n",
    "    'group_id': 830954892, \n",
    "    'sender': {'age': 0, 'area': '', 'card': '', 'level': '', 'nickname': 'ç¦»å²±å’Œè‘›é¥°åº”ä¸º', 'role': 'member', 'sex': 'unknown', 'title': '', 'user_id': 1352402688}, \n",
    "    'user_id': 1352402688, \n",
    "    'message_id': -185279243\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "ltp = LTP('LTP/small')\n",
    "message = 'æ˜å¤©å¤©æ€ä¹ˆæ ·å‘€'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.object_definition import Statement\n",
    "from service.MatchSys.utils import import_module\n",
    "\n",
    "\n",
    "class MessageAdapter(object):\n",
    "    \"\"\"\n",
    "    This is an abstract class that represents the interface\n",
    "    that all message adapters should implement.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        from service.MatchSys.object_definition import Statement\n",
    "        from ltp import LTP\n",
    "\n",
    "        # åˆå§‹åŒ–é¢„å¤„ç†ç¨‹åº\n",
    "        preprocessors = kwargs.get('preprocessors', ['jionlp.clean_text'])\n",
    "        self.preprocessors = []\n",
    "        for preprocessor in preprocessors:\n",
    "            self.preprocessors.append(import_module(preprocessor))\n",
    "        \n",
    "        model_path = kwargs.get('model_path', 'LTP/small')\n",
    "        self.ltp = LTP(model_path)\n",
    "    class AdapterMethodNotImplementedError(NotImplementedError):\n",
    "        \"\"\"\n",
    "        An exception to be raised when an adapter method has not been implemented.\n",
    "        Typically this indicates that the developer is expected to implement the\n",
    "        method in a subclass.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, message='This method must be overridden in a subclass method.'):\n",
    "            \"\"\"\n",
    "            Set the message for the exception.\n",
    "            \"\"\"\n",
    "            super().__init__(message)\n",
    "\n",
    "    def check(self, message):\n",
    "        if message is None or message == '':\n",
    "            raise self.ChatBotException(\n",
    "                'Either a statement object or a \"text\" keyword '\n",
    "                'argument is required. Neither was provided.'\n",
    "            )\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def process(self, message):\n",
    "        # read the message\n",
    "\n",
    "        # Get Text message\n",
    "        input_statement = self.text_process(text=message)\n",
    "\n",
    "        # Add Other Info\n",
    "\n",
    "        # è·å–Statement\n",
    "        raise self.AdapterMethodNotImplementedError()\n",
    "\n",
    "    def text_process(self, text,**kwargs):\n",
    "        \"\"\"Return Search Text\n",
    "\n",
    "        Args:\n",
    "            text (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: Statement\n",
    "        \"\"\"\n",
    "        # æ¸…ç†æ–‡æœ¬\n",
    "        for preprocessor in self.preprocessors:\n",
    "            text = preprocessor(text)\n",
    "        kwargs['text'] = text\n",
    "        # åˆ†è¯\n",
    "        result = self.ltp.pipeline('ä½ è§‰å¾—Aæ€ä¹ˆæ ·', tasks = [\"cws\",\"srl\"])\n",
    "        kwargs['search_text'] = result.cws\n",
    "        t = result.srl[0]\n",
    "        for item in result.srl:\n",
    "            if len(t['arguments']) > len(item['arguments']):\n",
    "                t = item\n",
    "        kwargs['intent'] = t\n",
    "\n",
    "        input_statement = Statement(**kwargs)\n",
    "        return input_statement\n",
    "\n",
    "    @property\n",
    "    def class_name(self):\n",
    "        \"\"\"\n",
    "        Return the name of the current logic adapter class.\n",
    "        This is typically used for logging and debugging.\n",
    "        \"\"\"\n",
    "        return str(self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMessageAdapter(MessageAdapter):\n",
    "    \"\"\"\n",
    "    This is an abstract class that represents the interface\n",
    "    that all message adapters should implement.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def process(self, message):\n",
    "        # read the message\n",
    "\n",
    "        # Get Text message\n",
    "        input_statement = self.text_process(text=message)\n",
    "\n",
    "        # Add Other Info\n",
    "\n",
    "        # è·å–Statement\n",
    "        return input_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_adapter = TextMessageAdapter(model_path=r'D:\\Code\\MyLongTimeProject\\A\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.trainers import Trainer\n",
    "from service.MatchSys.utils import print_progress_bar\n",
    "\n",
    "class QATrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows a chat bot to be trained using a list of strings\n",
    "    where the list represents a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, conversation, **kwargs):\n",
    "        \"\"\"\n",
    "        {Q:[A1,A2...]}\n",
    "        Train the chat bot based on the provided list of\n",
    "        statements that represents a single conversation.\n",
    "        \"\"\"\n",
    "        source = kwargs.get('source', 'TRAIN_DATA')\n",
    "        conversation_text = kwargs.get('conversation', 'TRAIN_DATA')\n",
    "        statements_to_create = []\n",
    "        for index,Q in enumerate(conversation):\n",
    "            if self.show_training_progress:\n",
    "                print_progress_bar(\n",
    "                    'QA Trainer',\n",
    "                    index + 1, len(conversation)\n",
    "                )\n",
    "    \n",
    "            statement = self.chatbot.message_adapter.process(Q)\n",
    "            statement.next_id=-statement.id\n",
    "            statement.conversation=conversation_text\n",
    "            statement.type_of='Q'\n",
    "            statement.source=source\n",
    "            statement.persona='user'\n",
    "            statements_to_create.append(statement)\n",
    "            for A in conversation[Q]:\n",
    "                statement = self.chatbot.message_adapter.process(A)\n",
    "                statement.previous_id=-statement.id\n",
    "                statement.conversation=conversation_text\n",
    "                statement.type_of='A'\n",
    "                statement.source=source\n",
    "                statement.persona='bot:'+self.chatbot.name\n",
    "                statements_to_create.append(statement)\n",
    "        self.chatbot.storage.create_many(statements_to_create)\n",
    "        self.chatbot.docvector_tool.train(statements_to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.match_sys import MatchSys\n",
    "\n",
    "\n",
    "matchsys = MatchSys(name='Alice',model_path=r'D:\\Code\\MyLongTimeProject\\A\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qatrain = QATrainer(matchsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv(r'data1.txt',\n",
    "                sep=\"$\",\n",
    "                engine='python'\n",
    "                )\n",
    "a.head(5)\n",
    "a.to_csv(r'data1.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':[1,2,3], 'b':[4,5,6], 'c':[7,8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,k in enumerate(a.items()):\n",
    "    print(index)\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "map = {}\n",
    "# data = pd.read_excel(r'C:\\Users\\Administrator\\Documents\\GitHub\\QQ-Bot-And-Tool\\data\\FixedReply\\å‚²å¨‡ç³»äºŒæ¬¡å…ƒbotè¯åº“5åƒè¯V1.2.xlsx',header=None, sheet_name=0)\n",
    "data = pd.read_excel(r\"data\\FixedReply\\Test-1K.xlsx\",header=None, sheet_name=0)\n",
    "for index,row in data.iterrows():\n",
    "    row[0] = str(row[0]).strip()\n",
    "    row[1] = str(row[1]).strip()\n",
    "    if row[0] in map:\n",
    "        map[row[0]].append(row[1])\n",
    "    else:\n",
    "        map[row[0]] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel(r\"data\\FixedReply\\Test-1K.xlsx\",header=None, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "# set HNSW index parameters\n",
    "M = 64  # number of connections each vertex will have\n",
    "ef_search = 32  # depth of layers explored during search\n",
    "ef_construction = 64  # depth of layers explored during index construction\n",
    "\n",
    "# initialize index (d == 128)\n",
    "index = faiss.IndexHNSWFlat(768, M)\n",
    "# set efConstruction and efSearch parameters\n",
    "index.hnsw.efConstruction = ef_construction\n",
    "index.hnsw.efSearch = ef_search\n",
    "# add data to index\n",
    "sentence_embeddings = np.load(\"data/vec_data.npy\")\n",
    "sentence_embeddings = sentence_embeddings.astype(np.float32)\n",
    "index.add(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import time\n",
    "\n",
    "class FaissSearch(object):\n",
    "    def __init__(self,**kwargs):\n",
    "        \n",
    "        self.vector_model = SentenceTransformer('data/text2vec-base-chinese')\n",
    "        self.nlist = 50\n",
    "        self.hnsw_index = None\n",
    "        self.signal_file_size = 10000\n",
    "        self.data_d = 128\n",
    "    def get_ids(self,data_list):\n",
    "        ids = []\n",
    "        for index,value in enumerate(data_list):\n",
    "            ids.append(index)\n",
    "        # å°†å‘é‡å¤„ç†ç»“æœå­˜å‚¨\n",
    "        ids_file = \"data/Faiss/ids_data.npy\"\n",
    "        np.save(ids_file, np.array(ids))\n",
    "        file_size = os.path.getsize(ids_file)\n",
    "        print(\"%7.3f MB\" % (file_size/1024/1024))\n",
    "    def data_process_10w(self,**kwargs):\n",
    "        data_save_path = kwargs.get('data_save_path',\"data/Faiss/vector_data\") \n",
    "        sentences = kwargs.get('data_list',None)\n",
    "        # åŠ è½½æ¨¡å‹ï¼Œå°†æ•°æ®è¿›è¡Œå‘é‡åŒ–å¤„ç†\n",
    "        sentence_embeddings = self.vector_model.encode(sentences)\n",
    "        # å°†å‘é‡å¤„ç†ç»“æœå­˜å‚¨\n",
    "        n, d = sentence_embeddings.shape\n",
    "        sentence_embeddings_file_name = n+\"-\"+d+\"-\"+time.time()+\"-vec_data.npy\"\n",
    "        np.save(os.path.join(data_save_path,sentence_embeddings_file_name), sentence_embeddings)\n",
    "        file_size = os.path.getsize(os.path.join(data_save_path,sentence_embeddings_file_name))\n",
    "        print(sentence_embeddings_file_name + \" %7.3f MB\" % (file_size/1024/1024))\n",
    "    def data_process_2_vector(self,**kwargs):\n",
    "        data_save_path = kwargs.get('data_save_path',\"data/Faiss/vector_data\") \n",
    "        data_list = kwargs.get('data_list',None)\n",
    "        file_list = kwargs.get('file_list',None)\n",
    "        if data_list:\n",
    "            # åŠ è½½æ¨¡å‹ï¼Œå°†æ•°æ®è¿›è¡Œå‘é‡åŒ–å¤„ç†\n",
    "            sentences = data_list\n",
    "            if len(sentences) <= 100000:\n",
    "                self.data_process_10w(**kwargs)\n",
    "            else:\n",
    "                for i in range(0, len(sentences), 100000):\n",
    "                    kwargs['data_list'] = sentences[i: i + 100000]\n",
    "                    self.data_process_10w(**kwargs)\n",
    "\n",
    "        elif file_list:\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\"\n",
    "    def build_hnsw(self,vec_path='data/Faiss/vec_data'):\n",
    "        sentence_embeddings_list = []\n",
    "        files = []\n",
    "        for file in os.listdir(vec_path):\n",
    "            if file.endswith('.npy'):\n",
    "                files.append(os.path.join(vec_path,file))\n",
    "                sentence_embeddings = np.load(os.path.join(vec_path,file))\n",
    "                sentence_embeddings = sentence_embeddings.astype(np.float32)\n",
    "                sentence_embeddings_list.append(sentence_embeddings)\n",
    "        \n",
    "        n,d = sentence_embeddings_list[0].shape\n",
    "        # set HNSW index parameters\n",
    "        M = 64  # number of connections each vertex will have è¶Šå¤§æœç´¢æ—¶é—´è¶Šé•¿\n",
    "        ef_search = 32  # depth of layers explored during search è¶Šå¤§æœç´¢æ—¶é—´è¶Šé•¿\n",
    "        ef_construction = 64  # depth of layers explored during index construction è¶Šå¤§æ„å»ºæ—¶é—´è¶Šé•¿\n",
    "\n",
    "        # initialize index (d == 128)\n",
    "        self.hnsw_index = faiss.IndexHNSWFlat(d, M)\n",
    "        # set efConstruction and efSearch parameters\n",
    "        self.hnsw_index.hnsw.efConstruction = ef_construction\n",
    "        self.hnsw_index.hnsw.efSearch = ef_search\n",
    "        # self.hnsw_index = faiss.IndexIDMap2(self.hnsw_index)\n",
    "        \n",
    "        self.add_data(vec_path)\n",
    "    def add_data(self,vec_path='data/Faiss/vec_data.npy'):\n",
    "        sentence_embeddings = np.load(vec_path)\n",
    "        sentence_embeddings = sentence_embeddings.astype(np.float32)\n",
    "        print(\"è½½å…¥å‘é‡æ•°æ®å®Œæ¯•ï¼Œæ•°æ®é‡\", len(sentence_embeddings))\n",
    "        self.hnsw_index.add(sentence_embeddings)\n",
    "    def search(self,sentences,topk=1):\n",
    "        if isinstance(sentences,list):\n",
    "            pass\n",
    "        else:\n",
    "            sentences = [sentences]\n",
    "        sentence_embeddings = self.vector_model.encode(sentences)\n",
    "        return self.hnsw_index.search(sentence_embeddings,topk)\n",
    "faissSearch = FaissSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17.156 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel(r\"data\\FixedReply\\å‚²å¨‡ç³»äºŒæ¬¡å…ƒbotè¯åº“5åƒè¯V1.2.xlsx\",header=None, sheet_name=0)\n",
    "faissSearch.data_process_2_vector(data_list=data[0].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è½½å…¥å‘é‡æ•°æ®å®Œæ¯•ï¼Œæ•°æ®é‡ 5856\n"
     ]
    }
   ],
   "source": [
    "faissSearch.add_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14981, 768)\n",
      "è½½å…¥å‘é‡æ•°æ®å®Œæ¯•ï¼Œæ•°æ®é‡ 14981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "faissSearch.build_hnsw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(faissSearch.hnsw_index,'data/Faiss/hnws.indx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.5935717e-10]], dtype=float32), array([[15024]], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faissSearch.search(['æˆ‘å–œæ¬¢ä½ '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½é¢„å¤„ç†æ•°æ®\n",
    "import numpy as np\n",
    "sentences = list(map.keys())\n",
    "sentence_embeddings = np.load(\"data/vec_data.npy\")\n",
    "print(\"è½½å…¥å‘é‡æ•°æ®å®Œæ¯•ï¼Œæ•°æ®é‡\", len(sentence_embeddings))\n",
    "\n",
    "# ä½¿ç”¨æ„å»ºå‘é‡æ—¶çš„æ¨¡å‹æ¥æ„å»ºå‘é‡ç´¢å¼•\n",
    "import faiss\n",
    "dimension = sentence_embeddings.shape[1]\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "nlist = 50\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "index.train(sentence_embeddings)\n",
    "index.add(sentence_embeddings)\n",
    "print(\"å»ºç«‹å‘é‡ç´¢å¼•å®Œæ¯•ï¼Œæ•°æ®é‡\", index.ntotal)\n",
    "\n",
    "# å°è¯•è¿›è¡ŒæŸ¥è¯¢\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('data/text2vec-base-chinese')\n",
    "print(\"è½½å…¥æ¨¡å‹å®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = 10\n",
    "search = model.encode([\"å¥½ç´¯å•Š\"])\n",
    "D, I = index.search(search, topK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in I[0]:\n",
    "    if i != -1:\n",
    "        ret = list(map.keys())[i]\n",
    "        print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"æ•°æ®ç±»å‹\",type(sentence_embeddings))           #æ‰“å°æ•°ç»„æ•°æ®ç±»å‹  \n",
    "print(\"æ•°ç»„å…ƒç´ æ•°æ®ç±»å‹ï¼š\",sentence_embeddings.dtype) #æ‰“å°æ•°ç»„å…ƒç´ æ•°æ®ç±»å‹  \n",
    "print(\"æ•°ç»„å…ƒç´ æ€»æ•°ï¼š\",sentence_embeddings.size)      #æ‰“å°æ•°ç»„å°ºå¯¸ï¼Œå³æ•°ç»„å…ƒç´ æ€»æ•°  \n",
    "print(\"æ•°ç»„å½¢çŠ¶ï¼š\",sentence_embeddings.shape)         #æ‰“å°æ•°ç»„å½¢çŠ¶  \n",
    "print(\"æ•°ç»„çš„ç»´åº¦æ•°ç›®\",sentence_embeddings.ndim)      #æ‰“å°æ•°ç»„çš„ç»´åº¦æ•°ç›®  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hnws.indx\n",
      "vec_data.npy\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('data/Faiss/'):\n",
    "    print(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
