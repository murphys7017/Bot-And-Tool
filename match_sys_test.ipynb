{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1714106437376024576\n",
      "Loading weights from local directory\n",
      "# jionlp - 微信公众号: JioNLP  Github: `https://github.com/dongrixinyu/JioNLP`.\n",
      "# jiojio - `http://www.jionlp.com/jionlp_online/cws_pos` is available for online trial.\n"
     ]
    }
   ],
   "source": [
    "from service.MatchSys.match_sys import MatchSys\n",
    "from service import config\n",
    "config.initialize()\n",
    "ms = MatchSys(\n",
    "        name='teat_sys',\n",
    "        ltp_model_path=r'D:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\data\\LtpModel',\n",
    "        database_uri='sqlite:///data/db.sqlite3',\n",
    "        text_vec_model_path=r'D:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\data\\model.pkl'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.trainers import Trainer\n",
    "from service.MatchSys.utils import print_progress_bar\n",
    "\n",
    "\n",
    "class QATrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows a chat bot to be trained using a list of strings\n",
    "    where the list represents a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, conversation, **kwargs):\n",
    "        \"\"\"\n",
    "        {Q:[A1,A2...]}\n",
    "        Train the chat bot based on the provided list of\n",
    "        statements that represents a single conversation.\n",
    "        \"\"\"\n",
    "        source = kwargs.get('source', 'TRAIN_DATA')\n",
    "        conversation_text = kwargs.get('conversation', 'TRAIN_DATA')\n",
    "        statements_to_create = []\n",
    "        previous_id = 0\n",
    "        next_id = 0\n",
    "        input_statements = self.chatbot.message_adapter.process_list(list(conversation.keys()),**kwargs)\n",
    "        for index,input_statement in enumerate(input_statements):\n",
    "                if self.show_training_progress:\n",
    "                    print_progress_bar(\n",
    "                        'QA Trainer',\n",
    "                        index + 1, len(conversation)\n",
    "                    )\n",
    "                try:\n",
    "                    statements_to_create.append(input_statement)\n",
    "                    kwargs['id'] = next_id\n",
    "                    kwargs['type_of'] = 'A'\n",
    "                    kwargs['persona'] = 'bot:'+self.chatbot.name\n",
    "                    for statement in  self.chatbot.message_adapter.process_list(conversation[input_statement.text],**kwargs):\n",
    "\n",
    "                        statements_to_create.append(statement)\n",
    "                except Exception as e:\n",
    "                    print(statement)\n",
    "        self.chatbot.storage.create_many(statements_to_create)\n",
    "        self.chatbot.docvector_tool.train(statements_to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "map = {}\n",
    "# data = pd.read_excel(r'C:\\Users\\Administrator\\Documents\\GitHub\\QQ-Bot-And-Tool\\data\\FixedReply\\傲娇系二次元bot词库5千词V1.2.xlsx',header=None, sheet_name=0)\n",
    "data = pd.read_excel(r\"D:\\temp\\Downloads\\傲娇系二次元bot词库5千词V1.2 - Test.xlsx\",header=None, sheet_name=0)\n",
    "for index,row in data.iterrows():\n",
    "    if row[0] in map:\n",
    "        map[row[0]].append(row[1])\n",
    "    else:\n",
    "        map[row[0]] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QATrainer(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Trainer: [####################] 100%\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\match_sys_test.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39mmap\u001b[39;49m)\n",
      "\u001b[1;32md:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\match_sys_test.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39mprint\u001b[39m(statement)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchatbot\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mcreate_many(statements_to_create)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchatbot\u001b[39m.\u001b[39;49mdocvector_tool\u001b[39m.\u001b[39;49mtrain(statements_to_create)\n",
      "File \u001b[1;32md:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\service\\MatchSys\\utils\\doc_vec_tool.py:31\u001b[0m, in \u001b[0;36mDoc2VecTool.train\u001b[1;34m(self, statements)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_model(statements)\n\u001b[0;32m     30\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_model(statements)\n",
      "File \u001b[1;32md:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\service\\MatchSys\\utils\\doc_vec_tool.py:45\u001b[0m, in \u001b[0;36mDoc2VecTool.train_model\u001b[1;34m(self, statements)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Doc2Vec\n\u001b[0;32m     43\u001b[0m tokenized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_tokenzied(statements)\n\u001b[1;32m---> 45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Doc2Vec(tokenized,dm\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, window\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, min_count\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain(tokenized,total_examples\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcorpus_count,epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:296\u001b[0m, in \u001b[0;36mDoc2Vec.__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m# advanced users should directly resize/adjust as desired after any vocab growth\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39mvectors_lockf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mREAL)  \u001b[39m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[39msuper\u001b[39m(Doc2Vec, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    297\u001b[0m     sentences\u001b[39m=\u001b[39mcorpus_iterable,\n\u001b[0;32m    298\u001b[0m     corpus_file\u001b[39m=\u001b[39mcorpus_file,\n\u001b[0;32m    299\u001b[0m     vector_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvector_size,\n\u001b[0;32m    300\u001b[0m     sg\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m dm) \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[0;32m    301\u001b[0m     null_word\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdm_concat,\n\u001b[0;32m    302\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    303\u001b[0m     window\u001b[39m=\u001b[39mwindow,\n\u001b[0;32m    304\u001b[0m     epochs\u001b[39m=\u001b[39mepochs,\n\u001b[0;32m    305\u001b[0m     shrink_windows\u001b[39m=\u001b[39mshrink_windows,\n\u001b[0;32m    306\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    307\u001b[0m )\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[0;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, total_examples\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs, start_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha, compute_loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:882\u001b[0m, in \u001b[0;36mDoc2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_vocab\u001b[39m(\n\u001b[0;32m    842\u001b[0m         \u001b[39mself\u001b[39m, corpus_iterable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, corpus_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, progress_per\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,\n\u001b[0;32m    843\u001b[0m         keep_raw_vocab\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, trim_rule\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    844\u001b[0m     ):\n\u001b[0;32m    845\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \n\u001b[0;32m    847\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m \n\u001b[0;32m    881\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m     total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_vocab(\n\u001b[0;32m    883\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file,\n\u001b[0;32m    884\u001b[0m         progress_per\u001b[39m=\u001b[39;49mprogress_per, trim_rule\u001b[39m=\u001b[39;49mtrim_rule,\n\u001b[0;32m    885\u001b[0m     )\n\u001b[0;32m    886\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n\u001b[0;32m    887\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words \u001b[39m=\u001b[39m total_words\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:1054\u001b[0m, in \u001b[0;36mDoc2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1052\u001b[0m     corpus_iterable \u001b[39m=\u001b[39m TaggedLineDocument(corpus_file)\n\u001b[1;32m-> 1054\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[0;32m   1056\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1057\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcollected \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m unique tags from a corpus of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m examples and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m words\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1058\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_vocab), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv), corpus_count, total_words,\n\u001b[0;32m   1059\u001b[0m )\n\u001b[0;32m   1061\u001b[0m \u001b[39mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:1007\u001b[0m, in \u001b[0;36mDoc2Vec._scan_vocab\u001b[1;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m doctags_list:\n\u001b[0;32m   1006\u001b[0m         doctags_lookup[key]\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m doctags_lookup[key]\u001b[39m.\u001b[39mindex \u001b[39m+\u001b[39m max_rawint \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1007\u001b[0m     doctags_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, max_rawint \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)) \u001b[39m+\u001b[39m doctags_list\n\u001b[0;32m   1009\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39mindex_to_key \u001b[39m=\u001b[39m doctags_list\n\u001b[0;32m   1010\u001b[0m \u001b[39mfor\u001b[39;00m t, dt \u001b[39min\u001b[39;00m doctags_lookup\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ms.get_response('你')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\match_sys_test.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ms\u001b[39m.\u001b[39mmessage_adapter\u001b[39m.\u001b[39msnowflake\u001b[39m.\u001b[39mget_id()):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     a\u001b[39m.\u001b[39mappend(i)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for i in range(ms.message_adapter.snowflake.get_id()):\n",
    "    a.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "conversation_text = kwargs.get('conversation', 'TRAIN_DATA')\n",
    "statements_to_create = []\n",
    "previous_id = 0\n",
    "next_id = 0\n",
    "kwargs['type_of'] = 'Q'\n",
    "kwargs['persona'] = 'user:*'\n",
    "input_statements = ms.message_adapter.process_list(list(map.keys()),**kwargs)\n",
    "for index,input_statement in enumerate(input_statements):\n",
    "    statements_to_create.append(input_statement)\n",
    "    kwargs['id'] = next_id\n",
    "    kwargs['type_of'] = 'A'\n",
    "    kwargs['persona'] = 'bot:'+ms.name\n",
    "\n",
    "data = ms.docvector_tool.build_tokenzied(statements_to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "count = 0\n",
    "for tag in data:\n",
    "    tag.tags[0] = count\n",
    "    temp.append(tag)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "ms.docvector_tool.model = Doc2Vec(temp,dm=1, window=8, min_count=5, workers=4)\n",
    "ms.docvector_tool.model.train(temp,total_examples=ms.docvector_tool.model.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import jieba\n",
    "# 加载数据\n",
    "documents = []\n",
    "# 使用count当做每个句子的“标签”，标签和每个句子是一一对应的\n",
    "count = 0\n",
    "for line in list(map.keys()):\n",
    "    documents.append(TaggedDocument(jieba.lcut_for_search(line), [str(line),count]))\n",
    "    count = count + 1\n",
    "# 模型训练\n",
    "model = Doc2Vec(documents, dm=1, window=8, min_count=5, workers=4)\n",
    "model.train(documents,total_examples=model.corpus_count,epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_vector = model.infer_vector(doc_words=['热', '死', '我', '了'])\n",
    "sims = model.dv.most_similar([inferred_vector],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sim in sims:\n",
    "    print(sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
