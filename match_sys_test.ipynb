{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1713847036446646272\n",
      "Loading weights from local directory\n",
      "# jionlp - 微信公众号: JioNLP  Github: `https://github.com/dongrixinyu/JioNLP`.\n",
      "# jiojio - `http://www.jionlp.com/jionlp_online/cws_pos` is available for online trial.\n"
     ]
    }
   ],
   "source": [
    "from service.MatchSys.match_sys import MatchSys\n",
    "from service import config\n",
    "config.initialize()\n",
    "ms = MatchSys(\n",
    "        name='teat_sys',\n",
    "        ltp_model_path=r'D:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\data\\LtpModel',\n",
    "        database_uri='sqlite:///data/db.sqlite3',\n",
    "        text_vec_model_path=r'D:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\data\\model.pkl'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from service.MatchSys.trainers import Trainer\n",
    "from service.MatchSys.utils import print_progress_bar\n",
    "\n",
    "\n",
    "class QATrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Allows a chat bot to be trained using a list of strings\n",
    "    where the list represents a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, conversation, **kwargs):\n",
    "        \"\"\"\n",
    "        {Q:[A1,A2...]}\n",
    "        Train the chat bot based on the provided list of\n",
    "        statements that represents a single conversation.\n",
    "        \"\"\"\n",
    "        source = kwargs.get('source', 'TRAIN_DATA')\n",
    "        conversation_text = kwargs.get('conversation', 'TRAIN_DATA')\n",
    "        statements_to_create = []\n",
    "        previous_id = 0\n",
    "        next_id = 0\n",
    "        input_statements = self.chatbot.message_adapter.process_list(list(conversation.keys()),**kwargs)\n",
    "        for index,input_statement in enumerate(input_statements):\n",
    "                if self.show_training_progress:\n",
    "                    print_progress_bar(\n",
    "                        'QA Trainer',\n",
    "                        index + 1, len(conversation)\n",
    "                    )\n",
    "                try:\n",
    "                    statements_to_create.append(input_statement)\n",
    "                    kwargs['id'] = next_id\n",
    "                    kwargs['type_of'] = 'A'\n",
    "                    kwargs['persona'] = 'bot:'+self.chatbot.name\n",
    "                    for statement in  self.chatbot.message_adapter.process_list(conversation[input_statement.text],**kwargs):\n",
    "\n",
    "                        statements_to_create.append(statement)\n",
    "                except Exception as e:\n",
    "                    print(statement)\n",
    "        self.chatbot.storage.create_many(statements_to_create)\n",
    "        self.chatbot.docvector_tool.train(statements_to_create)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "map = {}\n",
    "data = pd.read_excel(r\"D:\\temp\\Downloads\\傲娇系二次元bot词库5千词V1.2 - Test.xlsx\",header=None, sheet_name=0)\n",
    "for index,row in data.iterrows():\n",
    "    if row[0] in map:\n",
    "        map[row[0]].append(row[1])\n",
    "    else:\n",
    "        map[row[0]] = [row[1]]\n",
    "\n",
    "kwargs = {}\n",
    "statements_to_create = []\n",
    "previous_id = 0\n",
    "next_id = 0\n",
    "input_statements = ms.message_adapter.process_list(list(map.keys()),**kwargs)\n",
    "for index,input_statement in enumerate(input_statements):\n",
    "    statements_to_create.append(input_statement)\n",
    "    kwargs['id'] = next_id\n",
    "    kwargs['type_of'] = 'A'\n",
    "    kwargs['persona'] = 'bot:'+ms.name\n",
    "    for statement in  ms.message_adapter.process_list(map[input_statement.text],**kwargs):\n",
    "        statements_to_create.append(statement)\n",
    "\n",
    "ms.storage.create_many(statements_to_create)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "for statement in statements_to_create:\n",
    "    if statement.id > 0:\n",
    "        tokenized.append(TaggedDocument(statement.search_text.split(' '),tags=[statement.id,statement.text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\MyLongTimeProject\\A\\QQ-Bot-And-Tool\\match_sys_test.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Doc2Vec\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m Doc2Vec(documents\u001b[39m=\u001b[39;49mtokenized,epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/MyLongTimeProject/A/QQ-Bot-And-Tool/match_sys_test.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ms\u001b[39m.\u001b[39mdocvector_tool\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain(tokenized,total_examples\u001b[39m=\u001b[39mms\u001b[39m.\u001b[39mdocvector_tool\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcorpus_count,epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:296\u001b[0m, in \u001b[0;36mDoc2Vec.__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m# advanced users should directly resize/adjust as desired after any vocab growth\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39mvectors_lockf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mREAL)  \u001b[39m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[39msuper\u001b[39m(Doc2Vec, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    297\u001b[0m     sentences\u001b[39m=\u001b[39mcorpus_iterable,\n\u001b[0;32m    298\u001b[0m     corpus_file\u001b[39m=\u001b[39mcorpus_file,\n\u001b[0;32m    299\u001b[0m     vector_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvector_size,\n\u001b[0;32m    300\u001b[0m     sg\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m dm) \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[0;32m    301\u001b[0m     null_word\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdm_concat,\n\u001b[0;32m    302\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    303\u001b[0m     window\u001b[39m=\u001b[39mwindow,\n\u001b[0;32m    304\u001b[0m     epochs\u001b[39m=\u001b[39mepochs,\n\u001b[0;32m    305\u001b[0m     shrink_windows\u001b[39m=\u001b[39mshrink_windows,\n\u001b[0;32m    306\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    307\u001b[0m )\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[0;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, total_examples\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs, start_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha, compute_loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:882\u001b[0m, in \u001b[0;36mDoc2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_vocab\u001b[39m(\n\u001b[0;32m    842\u001b[0m         \u001b[39mself\u001b[39m, corpus_iterable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, corpus_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, progress_per\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,\n\u001b[0;32m    843\u001b[0m         keep_raw_vocab\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, trim_rule\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    844\u001b[0m     ):\n\u001b[0;32m    845\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \n\u001b[0;32m    847\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m \n\u001b[0;32m    881\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m     total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_vocab(\n\u001b[0;32m    883\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file,\n\u001b[0;32m    884\u001b[0m         progress_per\u001b[39m=\u001b[39;49mprogress_per, trim_rule\u001b[39m=\u001b[39;49mtrim_rule,\n\u001b[0;32m    885\u001b[0m     )\n\u001b[0;32m    886\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n\u001b[0;32m    887\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words \u001b[39m=\u001b[39m total_words\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:1054\u001b[0m, in \u001b[0;36mDoc2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1052\u001b[0m     corpus_iterable \u001b[39m=\u001b[39m TaggedLineDocument(corpus_file)\n\u001b[1;32m-> 1054\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[0;32m   1056\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1057\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcollected \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m unique tags from a corpus of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m examples and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m words\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1058\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_vocab), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv), corpus_count, total_words,\n\u001b[0;32m   1059\u001b[0m )\n\u001b[0;32m   1061\u001b[0m \u001b[39mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32md:\\APP\\CodeUse\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:1007\u001b[0m, in \u001b[0;36mDoc2Vec._scan_vocab\u001b[1;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m doctags_list:\n\u001b[0;32m   1006\u001b[0m         doctags_lookup[key]\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m doctags_lookup[key]\u001b[39m.\u001b[39mindex \u001b[39m+\u001b[39m max_rawint \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1007\u001b[0m     doctags_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, max_rawint \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)) \u001b[39m+\u001b[39m doctags_list\n\u001b[0;32m   1009\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39mindex_to_key \u001b[39m=\u001b[39m doctags_list\n\u001b[0;32m   1010\u001b[0m \u001b[39mfor\u001b[39;00m t, dt \u001b[39min\u001b[39;00m doctags_lookup\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "model = Doc2Vec(documents=tokenized,epochs=50)\n",
    "ms.docvector_tool.model.train(tokenized,total_examples=ms.docvector_tool.model.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QATrainer(ms)\n",
    "trainer.train(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ms.get_response('你')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
