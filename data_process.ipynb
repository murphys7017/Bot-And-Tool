{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "names = os.listdir('è¯æ¡')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = {}\n",
    "for file in names:\n",
    "    data = pd.read_excel('è¯æ¡//'+file,header=None, sheet_name=0)\n",
    "    for index,row in data.iterrows():\n",
    "        if row[0] in map:\n",
    "            map[row[0]].append(row[1])\n",
    "        else:\n",
    "            map[row[0]] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import websocket\n",
    "import time\n",
    "import threading\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "def on_open(wsapp):\n",
    "\n",
    "    print(\"on_open\")\n",
    "\n",
    "\n",
    "def on_close(wsapp):\n",
    "    print(\"on_close\")\n",
    "\n",
    "\n",
    "\n",
    "wsapp = websocket.WebSocketApp(\"ws://127.0.0.1:8081\",\n",
    "                               on_open=on_open,\n",
    "                               on_close=on_close)\n",
    "wsapp.run_forever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'data': [\n",
    "    {'nickname': 'å…³ä¼¯å…°', 'remark': 'å…³ä¼¯å…°', 'user_id': 474527445}, \n",
    "    {'nickname': 'ğŸğŸğŸ', 'remark': 'ğŸğŸğŸ', 'user_id': 577003680}, \n",
    "], 'message': '', 'retcode': 0, 'status': 'ok'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.ç”¨æˆ·æ¶ˆæ¯è‡³æ§åˆ¶å™¨DispatcherServlet\n",
    "2.DispatcherServletè¿›è¡ŒåŠ å·¥å¤„ç†åè°ƒç”¨å¤„ç†å™¨æ˜ å°„å™¨HandlerMappingã€‚\n",
    "3.HandlerMappingæ ¹æ®æ¶ˆæ¯æ‰¾åˆ°å…·ä½“çš„å¤„ç†å™¨ï¼Œç”Ÿæˆå“åº”æ¶ˆæ¯è¿”å›ç»™DispatcherServletã€‚\n",
    "4.DispatcherServletæ ¹æ®å¤„ç†å™¨Handlerè·å–å¤„ç†å™¨é€‚é…å™¨HandlerAdapteræ‰§è¡ŒHandlerAdapterå¤„ç†ä¸€ç³»åˆ—çš„æ“ä½œï¼Œå¦‚ï¼šå‚æ•°å°è£…ï¼Œæ•°æ®æ ¼å¼è½¬æ¢ï¼Œæ•°æ®éªŒè¯ç­‰æ“ä½œ\n",
    "5.æ‰§è¡Œå¤„ç†å™¨Handler(Controllerï¼Œä¹Ÿå«é¡µé¢æ§åˆ¶å™¨)ã€‚\n",
    "6.Handleræ‰§è¡Œå®Œæˆè¿”å›ModelAndView\n",
    "7.HandlerAdapterå°†Handleræ‰§è¡Œç»“æœModelAndViewè¿”å›åˆ°DispatcherServlet\n",
    "8.DispatcherServletå°†ModelAndViewä¼ ç»™ViewResloverè§†å›¾è§£æå™¨\n",
    "9.ViewResloverè§£æåè¿”å›å…·ä½“View\n",
    "10.DispatcherServletå¯¹Viewè¿›è¡Œæ¸²æŸ“è§†å›¾ï¼ˆå³å°†æ¨¡å‹æ•°æ®modelå¡«å……è‡³è§†å›¾ä¸­ï¼‰ã€‚\n",
    "11.DispatcherServletå“åº”ç”¨æˆ·ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'post_type': 'message', \n",
    "    'message_type': 'private', \n",
    "    'time': 1694183059, \n",
    "    'self_id': 2762018040, \n",
    "    'sub_type': 'friend', \n",
    "    'target_id': 2762018040, \n",
    "    'message': 'ä½ å¥½', \n",
    "    'raw_message': 'ä½ å¥½', \n",
    "    'font': 0, \n",
    "    'sender': {'age': 0, 'nickname': 'Aki-Polaris', 'sex': 'unknown', 'user_id': 815049548}, \n",
    "    'message_id': -2001115448, \n",
    "    'user_id': 815049548\n",
    "}\n",
    "{\n",
    "    'post_type': 'message', \n",
    "    'message_type': 'group', \n",
    "    'time': 1694395091, \n",
    "    'self_id': 2762018040, \n",
    "    'sub_type': 'normal', \n",
    "    'anonymous': None, \n",
    "    'message': '[CQ:image,file=138fd15bdabbcfb4c3daa148555fe447.image,subType=1,url=https://gchat.qpic.cn/gchatpic_new/1352402688/830954892-2582910460-138FD15BDABBCFB4C3DAA148555FE447/0?term=255&amp;is_origin=0]', \n",
    "    'message_seq': 81567, \n",
    "    'raw_message': '[CQ:image,file=138fd15bdabbcfb4c3daa148555fe447.image,subType=1,url=https://gchat.qpic.cn/gchatpic_new/1352402688/830954892-2582910460-138FD15BDABBCFB4C3DAA148555FE447/0?term=255&amp;is_origin=0]', \n",
    "    'font': 0, \n",
    "    'group_id': 830954892, \n",
    "    'sender': {'age': 0, 'area': '', 'card': '', 'level': '', 'nickname': 'ç¦»å²±å’Œè‘›é¥°åº”ä¸º', 'role': 'member', 'sex': 'unknown', 'title': '', 'user_id': 1352402688}, \n",
    "    'user_id': 1352402688, \n",
    "    'message_id': -185279243\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "dirname, filename = os.path.split('a.py')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os_str = 'python Test.py'\n",
    "f = os.popen(os_str, 'r')\n",
    "res = f.readlines()\t\t# resæ¥å—è¿”å›ç»“æœ\n",
    "f.close()\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_map = {}\n",
    "thread_map['t1'] = {}\n",
    "thread_map['t1']['run'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "def bk(s):\n",
    "    print(s)\n",
    "scheduler = BackgroundScheduler()\n",
    "job = scheduler.add_job(bk,args=['sssssssssss'],trigger='interval',seconds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "http_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')    # åŒ¹é…æ¨¡å¼\n",
    "file_pattern = re.compile(r'(?:[A-Z]:|\\\\\\\\|(?:\\\\.{1,2}[\\\\/\\\\\\\\])+)[\\\\w+\\\\\\\\\\\\s_\\\\(\\\\)\\\\/]+(?:\\\\.\\\\w+)*')\n",
    "string = 'Its after 12 noon, do you know where your rooftops are? http://tinyurl.com/NYCRooftops.jpg your rooftops are file///D:\\\\Code\\\\MyLongTimeProject\\\\A\\\\data\\\\images\\\\temp\\\\ '\n",
    "\n",
    "url = re.findall(file_pattern,string)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import jieba\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "def deal(list_ori,p):   \n",
    "    list_new=[]\t\t\t\t#å¤„ç†åçš„åˆ—è¡¨ï¼Œæ˜¯ä¸€ä¸ªäºŒç»´åˆ—è¡¨\n",
    "    list_short=[]\t\t\t#ç”¨äºå­˜æ”¾æ¯ä¸€æ®µåˆ—è¡¨\n",
    "    for i in list_ori:\n",
    "        if i!=p:\t\t\n",
    "            list_short.append(i)\n",
    "        else:\n",
    "            list_new.append(list_short)\n",
    "            list_short=[]\n",
    "    list_new.append(list_short)   #æœ€åä¸€æ®µé‡ä¸åˆ°åˆ‡å‰²æ ‡è¯†ï¼Œéœ€è¦æ‰‹åŠ¨æ”¾å…¥\n",
    "    return list_new\n",
    "\n",
    "class ComplexEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif isinstance(obj, date):\n",
    "            return obj.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "        \n",
    "class Parrot(object):\n",
    "    stopwords = []\n",
    "    model = object\n",
    "    history = []\n",
    "    def __init__(self,work_path):\n",
    "        file_list =  os.listdir(work_path)\n",
    "        if 'stopwords.txt' in file_list:\n",
    "            self.stopwords = [line.replace('\\n','',1) for line in open(work_path+'/stopwords.txt').readlines()]\n",
    "        if 'model.bin' in file_list:\n",
    "            self.model = Doc2Vec.load(work_path+'/model.bin')\n",
    "        if 'history.json' in file_list:\n",
    "            with open(work_path+'/history.json') as f:\n",
    "                self.history = json.load(f)\n",
    "\n",
    "    def remove_stopwords(self,str1):\n",
    "        words = []\n",
    "        for word in jieba.lcut(str1):\n",
    "            if word not in self.stopwords:\n",
    "                words.append(word)\n",
    "        return words\n",
    "\n",
    "    def process_chat_history(self,file_path):\n",
    "        pt = re.compile(r'(\\d{4}-\\d{1,2}-\\d{1,2} \\d{1,2}:\\d{1,2}:\\d{1,2}) ([^(]*)\\(([1-9][0-9]{4,})\\)')\n",
    "        msg_list = []\n",
    "        nicknames = []\n",
    "\n",
    "        file = open(file_path).readlines()\n",
    "        file = deal(file,'\\n')\n",
    "\n",
    "        for item in file:\n",
    "            try:\n",
    "                info = re.findall(pt,item[0])[0]\n",
    "                nicknames.append(json.dumps(info[1],cls=ComplexEncoder))\n",
    "                if '[å›¾ç‰‡]' not in item[1]:\n",
    "                    msg_list.append({\n",
    "                                        'time': datetime.strptime(info[0], \"%Y-%m-%d %H:%M:%S\"),\n",
    "                                        'nickname': info[1],\n",
    "                                        'qq': info[2],\n",
    "                                        'message': item[1].replace('\\n', ''),\n",
    "                                    })\n",
    "            except:\n",
    "                pass\n",
    "        jsont = json.dumps(msg_list,cls=ComplexEncoder)\n",
    "\n",
    "        for name in nicknames:\n",
    "            jsont = jsont.replace('@','')\n",
    "            jsont = jsont.replace(name.replace('\\\"',''),'')\n",
    "\n",
    "        jsont = json.loads(jsont)\n",
    "        msg_list = []\n",
    "        for item in jsont:\n",
    "            item['time'] = datetime.strptime(item['time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "            msg_list.append(item)\n",
    "        return msg_list\n",
    "    def train_model_history(self,history_path):\n",
    "        self.history = self.process_chat_history(history_path)\n",
    "        sentences = [self.remove_stopwords(item['message']) for item in self.history]\n",
    "        tokenized = [TaggedDocument(sentence,tags=[index]) for index,sentence in enumerate(sentences)]\n",
    "        self.model = Doc2Vec(tokenized,min_count=1,window=3,sample=1e-3,negative=5,workers=4)\n",
    "        self.model.train(tokenized,total_examples=self.model.corpus_count,epochs=10)\n",
    "    def train_model(self, data):\n",
    "        for line in data:\n",
    "            self.history.append({'message' : line})\n",
    "        sentences = [self.remove_stopwords(s) for s in data]\n",
    "        tokenized = [TaggedDocument(sentence,tags=[index]) for index,sentence in enumerate(sentences)]\n",
    "        self.model = Doc2Vec(tokenized,min_count=1,window=3,sample=1e-3,negative=5,workers=4)\n",
    "        self.model.train(tokenized,total_examples=self.model.corpus_count,epochs=10)\n",
    "    def save_model(self,save_path):\n",
    "        self.model.save(save_path+'/model.bin')\n",
    "        with open(save_path+'/history.json','w') as f:\n",
    "            json.dump(self.history,f,cls=ComplexEncoder)\n",
    "    def inferred2string(self,msg):\n",
    "        inferred_vector = self.model.infer_vector(doc_words=self.remove_stopwords(msg))\n",
    "        sims = self.model.dv.most_similar([inferred_vector],topn=1)[0][0]\n",
    "        return self.history[sims]['message']\n",
    "    def update_model(self,data):\n",
    "        \"\"\"æœ€å¥½æ˜¯{\n",
    "                    'time': \"%Y-%m-%d %H:%M:%S\",\n",
    "                    'nickname': info,\n",
    "                    qq': info,\n",
    "                    'message': item,\n",
    "                }\n",
    "            å…¶ä¸­messageä¸ºå¿…é¡»çš„\n",
    "        \"\"\"\n",
    "        for line in data:\n",
    "            self.history.append({'message' : line})\n",
    "        sentence = [self.remove_stopwords(s) for s in data]\n",
    "        tokenized = [TaggedDocument(sentence,tags=[index]) for index,sentence in enumerate(sentence)]\n",
    "        self.model.build_vocab(tokenized,update=True) #æ³¨æ„update = True è¿™ä¸ªå‚æ•°å¾ˆé‡è¦\n",
    "        self.model.train(tokenized,total_examples=self.model.corpus_count,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parrot = Parrot('./data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path =  './'\n",
    "list = []\n",
    "for file in os.listdir(data_path+'/FixedReply'):\n",
    "    data = pd.read_excel(data_path+'/FixedReply/'+file,header=None, sheet_name=0)\n",
    "    for index,row in data.iterrows():\n",
    "        list.append(str(row[0]))\n",
    "        list.append(str(row[1]))\n",
    "parrot.update_model(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parrot.save_model('./data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.888 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ç”¨â—‹â—‹å¸®æˆ‘'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = 'æœ€æ–°æ¬¾ç½‘çº¢è™¾å¤´å¥³å°ç©å…·'\n",
    "parrot.inferred2string(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = []\n",
    "temp = []\n",
    "for i in range(len(msg_list)-1):\n",
    "    t = msg_list[i+1]['time'] - msg_list[i]['time']\n",
    "    if t.total_seconds() < 60*5:\n",
    "        if '[å›¾ç‰‡]' not in msg_list[i]['message']:\n",
    "            temp.append(msg_list[i]['message'])\n",
    "    else:\n",
    "        if len(temp) > 3:\n",
    "            chats.append(temp)\n",
    "        temp = []\n",
    "chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "obj_list = {}\n",
    "index = 1\n",
    "for chat in chats:\n",
    "    obj_list[index] = {}\n",
    "    for i in range(len(chat)-1):\n",
    "        words = pseg.cut(chat[i])\n",
    "        obj_list[index][chat[i]] = []\n",
    "        for word, flag in words:\n",
    "            obj_list[index][chat[i]].append((word,flag))\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "class CqCodeBuilder(object):\n",
    "    HTTP_SERVER = ''\n",
    "    TEMP_FILE_DIR = ''\n",
    "    def __init__(self,url,temp_file):\n",
    "        self.TEMP_FILE_DIR = temp_file\n",
    "        if url.endswith('/'):\n",
    "            self.HTTP_SERVER = url\n",
    "        else:\n",
    "            self.HTTP_SERVER = url + '/'\n",
    "    # TODO: å¢å¼ºåŠŸèƒ½ï¼Œç°åœ¨åªèƒ½è®¤ä¸ºæ§åˆ¶url\n",
    "    def resource_download(self,url):\n",
    "        dirname, filename = os.path.split(url)\n",
    "        if '.' not in filename:\n",
    "            print(\"è¯·è°¨æ…åˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºæ–‡ä»¶ï¼Œå¦‚éæ–‡ä»¶å¯èƒ½é€ æˆæœªçŸ¥é”™è¯¯\")\n",
    "        if filename in os.listdir(self.TEMP_FILE_DIR):\n",
    "            return self.TEMP_FILE_DIR+filename\n",
    "        else:\n",
    "            r = requests.get(url)\n",
    "            with open(self.TEMP_FILE_DIR+filename, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "                return self.TEMP_FILE_DIR+filename\n",
    "    def SendAction(self,action,params):\n",
    "        \"\"\"Send Action\n",
    "        \"\"\"\n",
    "        url = self.HTTP_SERVER + action\n",
    "        response = requests.get(url, params=params)\n",
    "        return response.json()\n",
    "    def base_builder(self,type,data):\n",
    "        res = '[CQ:' + type\n",
    "        for key in data:\n",
    "            res = res + ',' + str(key) + '=' + str(data[key])\n",
    "        res = res + ']'\n",
    "        return res\n",
    "    def builder(self,message):\n",
    "        in_list = re.compile(r'[\\[](.*?)[\\]]')\n",
    "        in_list = re.findall(in_list, message)\n",
    "        for item in in_list:\n",
    "            message = message.replace('['+item+']', self.image(item))\n",
    "        return message\n",
    "\n",
    "    def image(self,url,is_flash=False,is_emoji=False):\n",
    "        data = {}\n",
    "        if is_flash:\n",
    "            data['type'] = 'flash'\n",
    "        if is_emoji:\n",
    "            data['subType'] = 1\n",
    "        if url:\n",
    "            dirname, filename = os.path.split(url)\n",
    "            data['file'] = filename\n",
    "            if url.startswith(\"http://\") or url.startswith(\"https://\"):\n",
    "                data['url'] = url\n",
    "            else:\n",
    "                data['url'] = 'file:///'+url\n",
    "            return self.base_builder('image',data)\n",
    "\n",
    "    def reply(self,reply,message_info):\n",
    "        data = {\n",
    "            'id': message_info['message_id'],\n",
    "            'qq': message_info['self_id'],\n",
    "            'time': int(round(time.time())*1000),\n",
    "            'seq': self.SendAction('get_msg',{'message_id': message_info['message_id']})['data']['message_seq']\n",
    "        }\n",
    "        return self.base_builder('reply',data) + reply\n",
    "\n",
    "    def poke(self,target):\n",
    "        return self.base_builder('poke',{'qq':target})\n",
    "    \n",
    "    def at(self,target):\n",
    "        return self.base_builder('at',{'qq':target})\n",
    "    \n",
    "    def record(self,file):\n",
    "        return self.base_builder('record',{'file':file})\n",
    "    def music(self,type,id):\n",
    "        return self.base_builder('music',{'type':type,'id':id})\n",
    "    def music_no_type(self,audio,title,image=None,content=None,url=None):\n",
    "        params = {\n",
    "            'type': 'custom',\n",
    "            'url': url,\n",
    "            'audio': audio,\n",
    "            'title': title,\n",
    "            'content': content,\n",
    "            'image': image\n",
    "\n",
    "        }\n",
    "        return self.base_builder('music_no_type',params)\n",
    "\n",
    "cqB = CqCodeBuilder('http://localhost:8882/','D:\\\\Code\\\\MyLongTimeProject\\\\A\\\\data\\\\images\\\\temp\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqB.music(r\"https://music.163.com/song?id=276294&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# è¿æ¥åˆ° Neo4j æ•°æ®åº“\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"123456\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "def add_question(question_text):\n",
    "    tokens = tokenize(question_text)\n",
    "    with driver.session() as session:\n",
    "        session.run(\n",
    "            \"MERGE (q:Question {text: $question_text, tokens: $tokens, count: 0})\",\n",
    "            question_text=question_text,\n",
    "            tokens=tokens\n",
    "        )\n",
    "\n",
    "def add_answer(answer_text, question_text):\n",
    "    tokens = tokenize(answer_text)\n",
    "    with driver.session() as session:\n",
    "        session.run(\n",
    "            \"MATCH (q:Question {text: $question_text}) \"\n",
    "            \"MERGE (a:Answer {text: $answer_text, tokens: $tokens, count: 0}) \"\n",
    "            \"CREATE (q)-[:ANSWERS]->(a)\",\n",
    "            question_text=question_text,\n",
    "            answer_text=answer_text,\n",
    "            tokens=tokens\n",
    "        )\n",
    "\n",
    "def get_answer_to_question(question_text, min_count=0):\n",
    "    tokens = tokenize(question_text)\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\n",
    "            \"MATCH (q:Question)-[:ANSWERS]->(a:Answer) \"\n",
    "            \"WHERE ALL(token in $tokens WHERE token in a.tokens) \"\n",
    "            \"AND q.text = $question_text \"\n",
    "            \"AND a.count >= $min_count \"\n",
    "            \"RETURN a.text\",\n",
    "            tokens=tokens,\n",
    "            question_text=question_text,\n",
    "            min_count=min_count\n",
    "        )\n",
    "        answers = [record[\"a.text\"] for record in result]\n",
    "        return answers\n",
    "\n",
    "# ä¾‹å­ï¼šæ·»åŠ é—®é¢˜å’Œå›ç­”ï¼Œå¹¶æé—®\n",
    "add_question(\"å¦‚ä½•ä½¿ç”¨ Python è¿æ¥ Neo4jï¼Ÿ\")\n",
    "add_answer(\"ä½ å¯ä»¥ä½¿ç”¨ neo4j åŒ…æ¥è¿æ¥ Neo4j æ•°æ®åº“ã€‚\", \"å¦‚ä½•ä½¿ç”¨ Python è¿æ¥ Neo4jï¼Ÿ\")\n",
    "\n",
    "# æé—®å¹¶è·å–ç­”æ¡ˆ\n",
    "answers = get_answer_to_question(\"å¦‚ä½•ä½¿ç”¨ Python è¿æ¥ Neo4jï¼Ÿ\")\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = get_answer_to_question(\"å¦‚ä½•ä½¿ç”¨ Python è¿æ¥ Neo4jï¼Ÿ\")\n",
    "print(answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
